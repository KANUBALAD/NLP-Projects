# -*- coding: utf-8 -*-
"""Fake or Real News Sample.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AsU0S8ZVNQN2QM8LoD1kJdXKAm74rI7e
"""

!pip install swifter

from google.colab import drive
drive.mount('/content/gdrive/')

### import google drive in order to access the dataset

# import necessary packages:

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score,classification_report

# read both csv files [true and fake datasets] 

fake = pd.read_csv("/content/gdrive/MyDrive/Datasets/Fake or True News/Fake.csv")
true = pd.read_csv("/content/gdrive/MyDrive/Datasets/Fake or True News/True.csv")

# peak into data

fake.head(4)

# first passage
fake.text[0]

true.head(4)

true.text[0]

## include the labels for each dataset

fake["label"] = "fake"
true["label"] = "true"

fake.head(2)

true.head(2)

## merge the two dataframes
df_new = pd.concat([true, fake], axis=0)
df_new.head(3)

## function to clean dataset


import re 
def preprocess_text(sen):
    sen = str(sen)

    # Removing html tags
    sentence = remove_tags(sen)
    
    # Remove punctuations and numbers
    sentence = re.sub('[^a-zA-Z]', ' ', sentence)

    # Single character removal
    sentence = re.sub(r"\s+[a-zA-Z]\s+", ' ', sentence)

    # Removing multiple spaces
    sentence = re.sub(r'\s+', ' ', sentence)

    return sentence

TAG_RE = re.compile(r'<[^>]+>')

def remove_tags(text):
     return TAG_RE.sub('', text)

## text function on a single text
preprocess_text(df_new.text[14])

import swifter
sentences_df = df_new.text.swifter.apply(preprocess_text)

sentences_df = pd.DataFrame(sentences_df)
df_updated = pd.concat([df_new.label, sentences_df], axis=1)
df_updated.head(3)

## mix up the labels
df_final = df_updated.sample(frac=0.1, replace=True, random_state=1)
df_final.head(3)

## check for null values
df_final.isnull().sum()

df_final.label.value_counts()

sns.countplot(x="label",data=df_final)
plt.title("Count Plot of Fake or Real News")
plt.xlabel("Fake or Real News")
plt.ylabel("Counts")

from sklearn.preprocessing import LabelEncoder

label_encoding = LabelEncoder()
df_final['label'] = label_encoding.fit_transform(df_final['label'])

## split dataset into train and test
X = df_final["text"]
y = df_final['label']
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.20,random_state=42)

### check if the daat splits are in the right shape format
print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

# instantiate the naive model
naive_model = Pipeline([('vector',TfidfVectorizer(stop_words='english')),
    ('classifier',MultinomialNB())
])

naive_model

# use model to train dataset

naive_model.fit(X_train,y_train)

## use to make prediction

predictions = naive_model.predict(X_test)

# Check model performance
model_accuracy = accuracy_score(y_test, predictions)
print(f'Naive model has {model_accuracy.round(2)*100}% accuracy')
print('\n')
print(classification_report(y_test, predictions))